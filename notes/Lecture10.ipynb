{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: Text Mining and Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we're going to be combining two topics that are closely related to many of the data science we've already learned. The first, text mining, is a powerful set of tools for acquiring and working with textual data. The second, graph theory, helps us understand networked data (which can be found in almost any application area).\n",
    "\n",
    "## Application Area\n",
    "\n",
    "To give us a motivating example, imagine you're creating a student-led course for Cornell students in data science. You've had experience with various topics in data science, but due to the enormous scope of data science you're not sure how to organize the topics or where to begin.\n",
    "\n",
    "Luckily, there are resources devoted to data science on the web. One of them is Data Science Central (http://www.datasciencecentral.com), which contains thousands of articles on data science written by industry practitioners. (We recommend subscribing to their newsletter.) We'd like to utilize this existing resource to organize the course into topics for us, giving us clusters of documents that we can use for research.\n",
    "\n",
    "Our approach is going to be as follows:\n",
    "\n",
    "1. Scrape the Data Science Central website and collect a repository of articles.\n",
    "2. Parse these articles into a machine-interpretable representation that we can analyze.\n",
    "3. Link documents together that share a certain number of words in common.\n",
    "4. Perform clustering on the graph to find communities of linked articles. These are potential course topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "We're going to use two techniques: __crawling__ and __scraping__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "We now have lots of text at our disposal, but we should clean up this data before attempting to work with it. Much of the messiness comes from two factors which we'll try to control:\n",
    "\n",
    "1. Language follows a power-law distribution: a few words appear most of the time and most of the words only appear a few times. Examples of words that we see very often are \"the\", \"we\", and \"and\". This creates a skewed distribution of word frequencies and obscures what the documents are actually about.\n",
    "2. Words often come in many variations. Think of the word \"educate\", for example: its many forms include \"educate\", \"educated\", \"educational\", \"educates\", \"educating\", and so on. We want to associate all of these words with the same root concept.\n",
    "\n",
    "To get rid of these problems, we perform the following operations:\n",
    "\n",
    "* __Punctuation removal.__\n",
    "* __Stop word removal.__\n",
    "* __Lemmatization.__\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "__Bag of words__\n",
    "\n",
    "\n",
    "When we take each bag of words and convert it to a row vector, and combine these row vectors into a matrix, we obtain a __document-term matrix__, a powerful representation of a collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a network\n",
    "\n",
    "### Graph structures\n",
    "\n",
    "### Graph properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding communities"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
