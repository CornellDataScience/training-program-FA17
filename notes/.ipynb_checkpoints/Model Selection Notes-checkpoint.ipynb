{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, you, the students, have been introduced the major topics in machine learning, as well as the problems the aim to address:\n",
    "\n",
    "* Supervised\n",
    "    * Classification\n",
    "    * Regression\n",
    "* Unsupervised\n",
    "    * Cluster Analysis\n",
    "    * Dimensionality Redunction\n",
    "    \n",
    "You have also learned of a number of models (Linear Regression, KMeans/KNN, Logistic Regression, etc.) as a means to approach these objectives. Unfortunately, knowledge of models does not a  data scientist make. One has to have an understanding of when and where a particular model is effective, and perhaps more importantly, when that model is inferior to another at performing a given task. \n",
    "\n",
    "This is what we mean by model selection - one cannot merely throw models at a problem and settle for whatever mediocre prediction results from the endeavor. Rather, we, as data scientists, must aspire to find the greatest approximation for a true solution to a problem, know what models may best achieve that objective, and be able to measure our success in towards achieving this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let us import some packages that will be used throughout this section of notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import sklearn.model_selection as ms\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Model Works in Every Situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should come as no surprise that there is no such thing as a perfect model (at least, not one that has been discovered), no one-size-fits all approach to solving a problem.\n",
    "\n",
    "Therefore, in the following sections we will be discussing points of failure in models we have introduced to you so far in the course, conditions to consider when selecting your model and, finally, ways to adjust your chosen model so that it better suits the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Failure and Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will discuss the shortcomings of a number of models that have previously been introduced in this course, as a means of demonstrating that one must be aware of _context_ when choosing their models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Problems With Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Linear resgression and classification methods and techniques derived from them are widespread throughout the field of data science. Therefore, we would like to specify that when referring to linear regression in this section, we will specifically be referring to the issues with Ordinary Least Squares Regression.\n",
    "\n",
    "Linear Regression is a well worn technique in Data Science, and quite often it is the first regression model that most students will be introduced to. Hopefully, you are all familiar with this model, but those who aren't may refer to previous week's lecture notes. \n",
    "\n",
    "As a quick review - Linear regression operates by minimizing Mean Squared Error by fitting weights to each parameter in the model. In doing so, it fights a straight line to the data which is used to generate quantitative predictions.\n",
    "\n",
    "The first -- and likely the most significant -- shortcoming of Linear Regression lies in one of the assumptions that make it work: that there exists an underlying linear structure to the data. This issue alone renders Linear regression ineffective in a vast number of cases where this assumption does not hold true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above demonstrates this failure. On the left, you can see a case where Linear regression works fairly well, as there does seem to be a linear structure to thte data. On the right, however, the \"line of best fit\", deos not see ot fit very well at all - this is because there is no linear structure to the data.\n",
    "\n",
    "A more subtle issue lies in Linear regression's poor handling of colinearity; consider the case where two input variables rely on each other, or have a high covariance, and where both relate highly to target variable. Then when used in a Linear regression model, both input variables may recieve significant weights, despite the fact that no additional information is gained from using both variables. This only makes your model more complex and may lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why KNN is rarely used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Neighbors is another model which the reader should have come accross at this point in time. KNN is widely taught as one of the first classification methods presented to students, due largely to both its approachability and its prowess for demonstrating a number of topics integral to machine learning. As a student of this course, KNN may be one of the first things you consider when solving a classification problem. It may come as a surprise to you then, that KNN is actually quite rarely used in practice.\n",
    "\n",
    "As a refresher, a KNN model splits the data up into neighborhoods of k samples, each of which it colors based on a majority vote of the samples in that neighborhood. Test points are given the same class as the neighborhoods they are positioned in.\n",
    "\n",
    "This leads us quite naturally to the first issue with KNN. KNN assumes that points close together in space will tend to be of the same class, and so it takes a majority vote on a small area to create predictions. However, this means that KNN has some fairly severe density reqiurements in order for it to function effectively. If the data is too sparse, then the neighborhoods will be quite spread out - thus, a majority vote will be rather ineffective, as the points in the neighborhood cannot truly be considered close. Thus, our assumption that close together points tend to have the same class is irrelevant, seeing as the points in the dataset are not adequately close for this purpose.\n",
    "\n",
    "The next issue is that KNN does not work particularly well on unbounded data, or on datasets where the amount of data along the boundary is lacking. Keep in mind that K nearest neighbors uses a majority vote of the neighborhood of the k points nearest to it. This graph (shown below) demonstrates the k nearest neighbors of an effective use of the model, those of an unbounded model, and those of a model with insuffecient data on the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the second and third examples must go further afield to find it's \"neighbors\". Thus, once again, the assumption about KNN described above is once again irrelevant.\n",
    "\n",
    "The final issue we'll talk about should be familiar to those of you who have formal experience programming: the complexity of K Nearest Neighbors at Test time scales linearly to the amount of data you have in your training set. In other words, as the size of the training dataset grows, so too does the time it takes to predict new values using the model.\n",
    "\n",
    "Now, students who are familiar with algorithms may think that running in linear time is fairly good; after all it's the best you could hope for for many common algorithms in Computer Science. However, while this may be acceptable for training your model, testing should be near instant for use in production code. To this end, KNN simply fails to deliver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Decision Trees and Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEEDS TO BE LOOKED INTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Best Way to Tell,  is to Test it Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, it proves quite difficult to keep track of every model and their respctive shortcomings. Therefore, there may be times where you'd like to use a model without exactly knowing how well it will work. As such, the best way to tell if a model does or doesn't work... is to simply test it out and see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to consider when choosing appropriate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Curse of Dimensionality sounds quite ominous, and has stricken despair into the hearts of many a data scientist. In short, the Curse is a catch-all term for the problems which arise from adding too many additional dimensions to your data:\n",
    "\n",
    "* Sparsity Increases - \n",
    "* Combinatorial Explosion - \n",
    "* Difficult to Identify Anomylous Data - \n",
    "* Increase in Data Requirements - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size and Complexity Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note should not be surprising. Different models take different amounts of space and time based a number of factors, and so one must be quite careful if working in a situation where memory or time is constrained - though a particular model may work better in a certain situation, one may sometimes need to choose the smaller, quicker option, if the constraints demand it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Underlying Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another point we have touched briefly on previously in this week's notes. One should consider the possibility that there is an underlying structure or trend to the data; some models perform very well or exclusively in the presence of such structure, while others tend to perform well in more general scenarios; therefore, one cannot neglect looking at and understanding their data and it's implications. Doing so could save serious headaches later on, and leads to more effective, more understood models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This point is simple, but certainly not one that should be overlooked. Though the amount of data available to data scientists is growing day by day, one needs to be aware of what they are working with on a case by case basis. It should be known that certiain models, such as Stocastic Gradient Descent, require quite a large amount of data in order to function properly. Being aware of the amount and quality of your data, as well as the implications of these restraints is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effectively Tuning Your Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why We Need To"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How We Go About It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approaching the Answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
